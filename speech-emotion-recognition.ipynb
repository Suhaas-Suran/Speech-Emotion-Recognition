{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Importing Modules\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-28T13:21:29.124541Z","iopub.execute_input":"2023-09-28T13:21:29.124913Z","iopub.status.idle":"2023-09-28T13:21:29.130150Z","shell.execute_reply.started":"2023-09-28T13:21:29.124885Z","shell.execute_reply":"2023-09-28T13:21:29.129136Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:32.483724Z","iopub.execute_input":"2023-09-28T14:21:32.484316Z","iopub.status.idle":"2023-09-28T14:21:34.808761Z","shell.execute_reply.started":"2023-09-28T14:21:32.484282Z","shell.execute_reply":"2023-09-28T14:21:34.806857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"code","source":"# Initialize lists to store file paths and labels\npaths = []\nlabels = []\n\n# Walk through the directory structure and collect file paths and labels\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        # Extract and preprocess labels from filenames\n        label = filename.split('_')[-1]\n        label = label.split('.')[0]\n        labels.append(label.lower())\n    # Limit the number of files to 2800 for loading efficiency\n    if len(paths) == 2800:\n        break\n\n# Print a message indicating that the dataset is loaded\nprint('Dataset is Loaded')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:34.821151Z","iopub.execute_input":"2023-09-28T14:21:34.823923Z","iopub.status.idle":"2023-09-28T14:21:35.262031Z","shell.execute_reply.started":"2023-09-28T14:21:34.823886Z","shell.execute_reply":"2023-09-28T14:21:35.260986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few file paths and labels\npaths[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.266761Z","iopub.execute_input":"2023-09-28T14:21:35.268460Z","iopub.status.idle":"2023-09-28T14:21:35.277535Z","shell.execute_reply.started":"2023-09-28T14:21:35.268422Z","shell.execute_reply":"2023-09-28T14:21:35.276152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.281913Z","iopub.execute_input":"2023-09-28T14:21:35.284431Z","iopub.status.idle":"2023-09-28T14:21:35.298283Z","shell.execute_reply.started":"2023-09-28T14:21:35.284395Z","shell.execute_reply":"2023-09-28T14:21:35.297028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame to store file paths and labels\ndf = pd.DataFrame()\ndf['speech'] = paths\ndf['label'] = labels\n\n# Display the first few rows of the DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.299642Z","iopub.execute_input":"2023-09-28T14:21:35.300631Z","iopub.status.idle":"2023-09-28T14:21:35.421907Z","shell.execute_reply.started":"2023-09-28T14:21:35.300536Z","shell.execute_reply":"2023-09-28T14:21:35.420751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Count and visualize the distribution of labels\ndf['label'].value_counts()\nsns.countplot(data=df, x='label')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.426538Z","iopub.execute_input":"2023-09-28T14:21:35.426883Z","iopub.status.idle":"2023-09-28T14:21:35.876906Z","shell.execute_reply.started":"2023-09-28T14:21:35.426848Z","shell.execute_reply":"2023-09-28T14:21:35.874181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to plot the waveform of audio data\ndef waveplot(data, sr, emotion):\n    plt.figure(figsize=(10, 4))\n    plt.title(emotion, size=20)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n\n# Define a function to plot the spectrogram of audio data\ndef spectrogram(data, sr, emotion):\n    x = librosa.stft(data)\n    xdb = librosa.amplitude_to_db(abs(x))\n    plt.figure(figsize=(11, 4))\n    plt.title(emotion, size=20)\n    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.colorbar()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.880683Z","iopub.execute_input":"2023-09-28T14:21:35.880969Z","iopub.status.idle":"2023-09-28T14:21:35.893417Z","shell.execute_reply.started":"2023-09-28T14:21:35.880943Z","shell.execute_reply":"2023-09-28T14:21:35.892277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'fear'\npath = np.array(df['speech'][df['label'] == emotion])[0]\ndata, sampling_rate = librosa.load(path)\n\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:35.894956Z","iopub.execute_input":"2023-09-28T14:21:35.895531Z","iopub.status.idle":"2023-09-28T14:21:47.247978Z","shell.execute_reply.started":"2023-09-28T14:21:35.895499Z","shell.execute_reply":"2023-09-28T14:21:47.247125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'angry'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:47.249420Z","iopub.execute_input":"2023-09-28T14:21:47.250652Z","iopub.status.idle":"2023-09-28T14:21:48.251780Z","shell.execute_reply.started":"2023-09-28T14:21:47.250612Z","shell.execute_reply":"2023-09-28T14:21:48.250756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'disgust'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:48.256537Z","iopub.execute_input":"2023-09-28T14:21:48.258484Z","iopub.status.idle":"2023-09-28T14:21:49.361963Z","shell.execute_reply.started":"2023-09-28T14:21:48.258449Z","shell.execute_reply":"2023-09-28T14:21:49.361109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'neutral'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:49.363448Z","iopub.execute_input":"2023-09-28T14:21:49.364538Z","iopub.status.idle":"2023-09-28T14:21:50.584748Z","shell.execute_reply.started":"2023-09-28T14:21:49.364505Z","shell.execute_reply":"2023-09-28T14:21:50.581622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'sad'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:50.586237Z","iopub.execute_input":"2023-09-28T14:21:50.587271Z","iopub.status.idle":"2023-09-28T14:21:51.608032Z","shell.execute_reply.started":"2023-09-28T14:21:50.587235Z","shell.execute_reply":"2023-09-28T14:21:51.607135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'ps'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:51.609585Z","iopub.execute_input":"2023-09-28T14:21:51.610735Z","iopub.status.idle":"2023-09-28T14:21:52.663094Z","shell.execute_reply.started":"2023-09-28T14:21:51.610688Z","shell.execute_reply":"2023-09-28T14:21:52.662163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion = 'happy'\npath = np.array(df['speech'][df['label']==emotion])[0]\ndata, sampling_rate = librosa.load(path)\nwaveplot(data, sampling_rate, emotion)\nspectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:52.664827Z","iopub.execute_input":"2023-09-28T14:21:52.665553Z","iopub.status.idle":"2023-09-28T14:21:53.749043Z","shell.execute_reply.started":"2023-09-28T14:21:52.665512Z","shell.execute_reply":"2023-09-28T14:21:53.748129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Define a function to extract MFCC features from an audio file\ndef extract_mfcc(filename):\n    y, sr = librosa.load(filename, duration=3, offset=0.5)\n    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n    return mfcc","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:53.750553Z","iopub.execute_input":"2023-09-28T14:21:53.751685Z","iopub.status.idle":"2023-09-28T14:21:53.758087Z","shell.execute_reply.started":"2023-09-28T14:21:53.751641Z","shell.execute_reply":"2023-09-28T14:21:53.757069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example: Extracting MFCC features for the first file\nextract_mfcc(df['speech'][0])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:53.759507Z","iopub.execute_input":"2023-09-28T14:21:53.760575Z","iopub.status.idle":"2023-09-28T14:21:54.834033Z","shell.execute_reply.started":"2023-09-28T14:21:53.760542Z","shell.execute_reply":"2023-09-28T14:21:54.832423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply MFCC extraction to all audio files in the DataFrame\nX_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))\nX_mfcc","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:21:54.836014Z","iopub.execute_input":"2023-09-28T14:21:54.836433Z","iopub.status.idle":"2023-09-28T14:23:21.398923Z","shell.execute_reply.started":"2023-09-28T14:21:54.836395Z","shell.execute_reply":"2023-09-28T14:23:21.397721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the MFCC data to a numpy array\nX = [x for x in X_mfcc]\nX = np.array(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:23:21.404907Z","iopub.execute_input":"2023-09-28T14:23:21.408317Z","iopub.status.idle":"2023-09-28T14:23:21.421805Z","shell.execute_reply.started":"2023-09-28T14:23:21.408242Z","shell.execute_reply":"2023-09-28T14:23:21.420698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the MFCC data\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:23:21.427388Z","iopub.execute_input":"2023-09-28T14:23:21.430587Z","iopub.status.idle":"2023-09-28T14:23:21.442657Z","shell.execute_reply.started":"2023-09-28T14:23:21.430540Z","shell.execute_reply":"2023-09-28T14:23:21.441368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the data to add a channel dimension\nX = np.expand_dims(X, -1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-28T14:23:21.444142Z","iopub.execute_input":"2023-09-28T14:23:21.444669Z","iopub.status.idle":"2023-09-28T14:23:21.456989Z","shell.execute_reply.started":"2023-09-28T14:23:21.444639Z","shell.execute_reply":"2023-09-28T14:23:21.455578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform one-hot encoding for labels\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ny = enc.fit_transform(df[['label']])\ny = y.toarray()\ny.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the LSTM Model","metadata":{}},{"cell_type":"code","source":"# Import Keras libraries for building the neural network model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the neural network model\nmodel = Sequential([\n    LSTM(256, return_sequences=False, input_shape=(40, 1)),\n    Dropout(0.2),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(7, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Display model summary\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X, y, validation_split=0.2, epochs=50, batch_size=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the results","metadata":{}},{"cell_type":"code","source":"# Plot training history - accuracy\nepochs = list(range(50))\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, label='train accuracy')\nplt.plot(epochs, val_acc, label='val accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history - loss\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(epochs, loss, label='train loss')\nplt.plot(epochs, val_loss, label='val loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a Predictive System","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the data to remove the channel dimension\nX = np.squeeze(X, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize and train the RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred = rf_classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a classification report\nclass_report = classification_report(y_test, y_pred)\nprint(\"Classification Report:\\n\", class_report)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to predict emotion from a new audio file\ndef predict_emotion(audio_file_path):\n    y, sr = librosa.load(audio_file_path, duration=3, offset=0.5)\n    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n    predicted_emotion = rf_classifier.predict([mfcc])  # Pass a 2D array with one sample\n    return predicted_emotion[0]\n\n# Example usage:\n# Replace 'your_audio_file.wav' with the path to your audio file\n# predicted_emotion = predict_emotion('your_audio_file.wav')\n# print(f\"Predicted Emotion: {predicted_emotion}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}